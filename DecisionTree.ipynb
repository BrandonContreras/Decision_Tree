{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal API of Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-ea7206607fc0>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-ea7206607fc0>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    my_tree.score(X, y[, sample_weight]) # Return the mean accuracy on the given test data and labels.\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"file.csv\")\n",
    "train, test = train_test_split(df, test_size=0.2) # find actual one to use\n",
    "\n",
    "\n",
    "my_tree = DecisionTree()\n",
    "my_tree.fit(X, Y) # X being all data, Y being just labels for each sample\n",
    "my_tree.predict(X) #---> return an array of target labels for each i in X (find shape of X at first)\n",
    "my_tree.score(X, y[, sample_weight]) # Return the mean accuracy on the given test data and labels.\n",
    "\n",
    "my_tree.plot() # returns graph visual of nodes\n",
    "my_tree.depth() # return max depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Notes:\n",
    "\n",
    "- How to handle a column that is not categorical? That is numerical\n",
    "- Create Method to determine type of column.\n",
    "- Change fit to take in X vectors and Y target vector\n",
    "- Convert to some other python tree for easy printing or something. Grahpical\n",
    "- Incorportate the Deku Tree for site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Site Posting Notes:\n",
    "- Use stanford kids site as rough guide but make cooler with Guess who. Start with that.\n",
    "- Note I chose more verbose programming statements in key parts of the algorithm as 1. More clear what part of the algo I'm doing. This is for me to learn, explain, and for others to easily see. If this were production I would be more likely to use more efficient python one liners. Have an example of a crazy one liner of like gini impurity.\n",
    "\n",
    "- For each method, have latex like 436 of math algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    #  Class attribute. Same value for every instance of this class.\n",
    "    numMoney = 100000\n",
    "\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.gini = None\n",
    "        self.left_child = None \n",
    "        self.right_child = None\n",
    "        self.class_label = None \n",
    "        self.is_leaf = False\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"NumSamples: {self.num_samples}\" \\\n",
    "        f\"\\nLabel: {self.class_label}\" \\\n",
    "        f\"\\nGini: {self.gini}\" \\\n",
    "        f\"\\nColumnToSplit: {self.split_feature}\" \\\n",
    "        f\"\\nSplitValue: {self.split_value}\" \\\n",
    "        f\"\\nLeaf: {self.is_leaf}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "        self.root_node = None\n",
    "        self.num_nodes = 0\n",
    "        self.target_feature = None\n",
    "        self.max_depth = max_depth\n",
    "        self.node_count = 0\n",
    "\n",
    "    def predict(self, sample):\n",
    "        \"\"\"Predict the class label of the given sample.\"\"\"\n",
    "        node = self.root_node\n",
    "        \n",
    "        while(node.is_leaf == False):\n",
    "            \n",
    "            if sample[node.split_feature] == node.split_value:\n",
    "                node = node.left_child\n",
    "            else:\n",
    "                node = node.right_child\n",
    "        return node.class_label\n",
    "\n",
    "    def _gini(self, df):\n",
    "        \"\"\"Returns gini score of df\n",
    "        uses target_feature variable of tree to find score\n",
    "        takes in a DF right now.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        t = len(df)\n",
    "        p = df.groupby(self.target_feature).size() / t\n",
    "        return 1 - sum(p**2)\n",
    "\n",
    "    def _impurity(self, df, prop):\n",
    "        s = 0\n",
    "        for k, subf in df.groupby(prop):\n",
    "            g = self._gini(subf)\n",
    "\n",
    "            # Get weighted average of gini values for a whole columns different values.\n",
    "            s += g * (len(subf) / len(df))\n",
    "        return s\n",
    "\n",
    "    def _split_feature(self, df):\n",
    "        \"\"\"Find the best feature to split on given a df. Return column to split on\"\"\"\n",
    "        # make better with just two lists? or a list of lists?\n",
    "        col_scores = {}\n",
    "        data = df.drop(self.target_feature, axis=1)\n",
    "\n",
    "        for column in data.columns:\n",
    "            weighted_gini = self._impurity(df, column)\n",
    "            col_scores[column] = weighted_gini\n",
    "\n",
    "        # return col_scores\n",
    "        # return (min(col_scores, key=col_scores.get))\n",
    "\n",
    "        lowest_feature = min(col_scores, key=col_scores.get)\n",
    "        lowest_score = col_scores[lowest_feature]\n",
    "\n",
    "        return lowest_feature, lowest_score\n",
    "\n",
    "    def _split_value(self, df, feature):\n",
    "        \"\"\"Find the best value of a given feature to split on\"\"\"\n",
    "\n",
    "        values = {}\n",
    "        for k, subf in df.groupby(feature):\n",
    "            g = self._gini(subf)\n",
    "            # Weighted average of gini score.\n",
    "            values[k] = g * len(subf) / len(df)\n",
    "\n",
    "        # return values\n",
    "        # return value that has the lowest gini score\n",
    "        return (min(values, key=values.get))\n",
    "\n",
    "    def fit(self, df, target):\n",
    "        \"\"\"where target is string of target column to predict.\"\"\"\n",
    "        self.target_feature = target\n",
    "        root_node = DecisionNode(len(df))\n",
    "\n",
    "    def _build_tree(self, df):\n",
    "        self.root_node = DecisionNode(len(df))\n",
    "\n",
    "        # Recursively add nodes to tree\n",
    "        self._splitNode(df, self.root_node, 0)\n",
    "\n",
    "    def _splitNode(self, df, node, current_level):\n",
    "        \"\"\"Given a df, split that into two subframes based on the best gini-gain\"\"\"\n",
    "        self.node_count += 1\n",
    "\n",
    "        feature, feature_score = self._split_feature(df)\n",
    "        node.gini = self._gini(df)\n",
    "        node.class_label = df[self.target_feature].mode()[0]\n",
    "\n",
    "\n",
    "      #  weighted_impurity_decrease = (\n",
    "       #     node.num_samples / self.root_node.num_samples) * (node.gini - feature_score)\n",
    "\n",
    "        # if ((node.gini - feature_score) > self.threshold ):\n",
    "\n",
    "        # if(node.gini > feature_score):\n",
    "        if(current_level < self.max_depth):\n",
    "            current_level += 1\n",
    "\n",
    "            node.is_leaf = False\n",
    "            node.split_feature = feature\n",
    "            node.split_value = self._split_value(df, feature)\n",
    "\n",
    "            left_df = df[df[feature] == node.split_value]  # left == true\n",
    "            right_df = df[df[feature] != node.split_value]  # right == false\n",
    "            \n",
    "            if(len(left_df) > 0):\n",
    "                node.left_child = DecisionNode(len(left_df))\n",
    "                self._splitNode(left_df, node.left_child, current_level)\n",
    "            if(len(right_df) > 0):\n",
    "                node.right_child = DecisionNode(len(right_df))\n",
    "                self._splitNode(right_df, node.right_child, current_level)\n",
    "            return\n",
    "        else:\n",
    "            node.is_leaf = True\n",
    "            return\n",
    "\n",
    "    def score(self, test):\n",
    "        \"\"\"Return accuracy on given samples and labels\"\"\"\n",
    "        result = pd.Series(dtype=object)\n",
    "        results = test.apply(lambda x: True if x[self.target_feature] == self.predict(x) else False, axis=1)\n",
    "        score = ( results.value_counts()[True] / len(results) ) * 100\n",
    "        return score\n",
    "      \n",
    "\n",
    "    def print_nodes(self):\n",
    "        \"\"\"Print tree in level order\"\"\"\n",
    "        if self.root_node == None:\n",
    "            return\n",
    "\n",
    "        queue = deque()\n",
    "        queue.append(self.root_node)\n",
    "\n",
    "        while(len(queue) > 0):\n",
    "            node = queue.popleft()\n",
    "            print(node)\n",
    "\n",
    "            if node.left_child != None:\n",
    "                queue.append(node.left_child)\n",
    "            if node.right_child is not None:\n",
    "                queue.append(node.right_child)\n",
    "\n",
    "    # if category is numerical, split into quartiles, get scores of each chunk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Draft below:\n",
    "# CURRENT OBJECTIVE:\n",
    "* Gini method with numpy DONE\n",
    "* Impurity method with numpy \n",
    "  \n",
    "  \n",
    "### New Gini Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class DecisionNode:\n",
    "\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.gini = None\n",
    "        self.left_child = None \n",
    "        self.right_child = None\n",
    "        self.class_label = None \n",
    "        self.is_leaf = False\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"NumSamples: {self.num_samples}\" \\\n",
    "        f\"\\nLabel: {self.class_label}\" \\\n",
    "        f\"\\nGini: {self.gini}\" \\\n",
    "        f\"\\nColumnToSplit: {self.split_feature}\" \\\n",
    "        f\"\\nSplitValue: {self.split_value}\" \\\n",
    "        f\"\\nLeaf: {self.is_leaf}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "        self.root_node = None\n",
    "        self.num_nodes = 0\n",
    "        self.max_depth = max_depth\n",
    "        self.node_count = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        fit builds the decision tree from the given training samples and their \n",
    "        respective labels\n",
    "\n",
    "        : array X: An array of training input samples of shape(n_samples, n_features)\n",
    "        : array y: An array representing the labels for each sample of shape(n_samples)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def gini_impurity(self, y):\n",
    "        \"\"\"\n",
    "        Returns the gini score of the array \n",
    "        TODO: THIS WORKS. WAS TESTED.  \n",
    "        1 FEEDS TO IMPURITY NEXT ->\n",
    "\n",
    "        : array y: Array containing labels\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        # labels: array containing all unique labels in y\n",
    "        # freq: where freq[i] = frequency of lables[i]\n",
    "        labels, freq = np.unique(y, return_counts=True)\n",
    "\n",
    "        # prob: performs vector wise division. Is a vector\n",
    "        prob = freq / n\n",
    "\n",
    "        # Vector wise squaring of the probabilites\n",
    "        prob = prob**2\n",
    "        # Add them all up\n",
    "        sigma_prob = prob.sum()\n",
    "\n",
    "        gini = 1 - sigma_prob\n",
    "        return gini\n",
    "\n",
    "\n",
    "    def category_impurity(self, split_feature, y_labels):\n",
    "        \"\"\"\n",
    "        Finds the total gini impurity for a possible split on the given vector.\n",
    "\n",
    "        :np.array split_feature: Vector of categorical feature values\n",
    "        :np.array y_labels: Vector of labels corresponding to split_feature\n",
    "        \"\"\"\n",
    "        n = len(y_labels)\n",
    "        # If I divide x_feature column up into groups for each of its different values, what is the gini score for each one\n",
    "        total_impurity = 0\n",
    "\n",
    "        # All the unique values of the proposed feature column\n",
    "        feature_values = np.unique(split_feature)\n",
    "\n",
    "        # For each  unique value in the feature column, find the weighted gini impurity of dividing the sample space\n",
    "        # into groups of that value, return the total impurity for all groups.\n",
    "\n",
    "        for value in feature_values:\n",
    "            y_group = y_labels[np.where(split_feature == value)]\n",
    "\n",
    "            gini_val = self.gini_impurity(y_group)\n",
    "\n",
    "            weighted_impurity = gini_val * (len(y_group) / n)\n",
    "\n",
    "            total_impurity += weighted_impurity\n",
    "\n",
    "        return total_impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CURRENT METHOD WORKING ON\n",
    "- Will i need 2 versions of this for categorical and numerical? or have boolean option for that\n",
    "- How to find the total gini impurity for a feature column if the values are NUMERICAL!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_impurity(split_feature, y_labels):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 30])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(['cat', 'cat', 'dog'])\n",
    "y = np.array([20, 30, 0])\n",
    "\n",
    "z = y[np.where(x == 'cat')]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Gain: 0.08333333333333348\n"
     ]
    }
   ],
   "source": [
    "initial = np.array(['red', 'red', 'red', 'blue', 'blue', 'blue', 'green', 'green', 'green'])\n",
    "data = np.array([1, 2, 2,2,2,2,2,2,2])\n",
    "total = gini_impurity(initial)\n",
    "\n",
    "possible = category_impurity(data, initial)\n",
    "\n",
    "print(f\"Gini Gain: {total - possible}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD VERSION OF METHOD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this but with numerical data.\n",
    "def category_impurity(split_feature, y_labels):\n",
    "    \"\"\"\n",
    "    Finds the total gini impurity for a possible split on the given vector.\n",
    "    \n",
    "    :np.array split_feature: Vector of categorical feature values\n",
    "    :np.array y_labels: Vector of labels corresponding to split_feature\n",
    "    \"\"\"\n",
    "    n = len(y_labels)\n",
    "    # If I divide x_feature column up into groups for each of its different values, what is the gini score for each one\n",
    "    total_impurity = 0\n",
    "    \n",
    "    # All the unique values of the proposed feature column\n",
    "    feature_values = np.unique(split_feature)\n",
    "    \n",
    "    # For each  unique value in the feature column, find the weighted gini impurity of dividing the sample space \n",
    "    # into groups of that value, return the total impurity for all groups.\n",
    "    \n",
    "    for value in feature_values:\n",
    "        y_group = y_labels[np.where(split_feature == value)]\n",
    "        \n",
    "        gini_val = gini_impurity(y_group)\n",
    "        \n",
    "        weighted_impurity = gini_val * (len(y_group) / n )\n",
    "        \n",
    "        total_impurity += weighted_impurity\n",
    "        \n",
    "    return total_impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## documentation\n",
    "def test_function(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    test_function does blah blah blah.\n",
    "\n",
    "    :param p1: describe about parameter p1\n",
    "    :param p2: describe about parameter p2\n",
    "    :param p3: describe about parameter p3\n",
    "    :return: describe what it returns\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very good query, like sql\n",
    "df_low = df.loc[df[\"salary\"]<6000,\"salary\"]\n",
    "\n",
    "# Lambda usuage\n",
    "df['age']=df.apply(lambda x: x['age']+3,axis=1)\n",
    "\n",
    "#Get a single value. Goes [row][column]\n",
    "sub_df.iloc[0]['A']\n",
    "\n",
    "# lambda\n",
    "efficiency_df[\"Speed Multiplier\"] = efficiency_df.apply(lambda x: x[\"Time(s)\"] / efficiency_df.iloc[0][\"Time(s)\"], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(self, df):\n",
    "    t = len(df)\n",
    "    p = df.groupby('class').size() / t\n",
    "    return sum(p * np.log2(p.values))\n",
    "\n",
    "\n",
    "def gini(self, df):  # performs gini on just one set, the 'class' column of mushroom df. scikit does not like strings\n",
    "    # scikit does not like categorical, must turn to enumeration if column is a string.\n",
    "    # this is the gini score if were to split this df on just this feature. again, lower is better.\n",
    "    t = len(df)\n",
    "    # should return series of length for each category. Thats why\n",
    "    # here class is a column of X, so groupby column index?\n",
    "\n",
    "    # group all columns of X by y-target value, then count them, then divide by the length of total X columns.\n",
    "    # this returns an ARRAY of values.\n",
    "\n",
    "    # count all y-column by how many x-rows have that certain value, count that.\n",
    "    p = df.groupby('class').size() / t\n",
    "    # sum is used below.\n",
    "    return 1 - sum(p**2)\n",
    "\n",
    "\n",
    "# returns the unique values for this categorical column.\n",
    "df.groupby('bruises').groups.keys()\n",
    "\n",
    "# how many samples of each category for feature 'odor'\n",
    "df.groupby('odor').size()\n",
    "\n",
    "for k, subf in df.groupby('bruises'):\n",
    "    # returns a subframe for each category of bruises values.\n",
    "    print(k, subf)\n",
    "\n",
    "# Here, we see if splitting the df by a certain feature helps get a better gini score\n",
    "# in relation to the 'class' variable, which is what we are trying to predict.\n",
    "for k, subf in df.groupby('cap-shape'):\n",
    "    # outputs 6 gini scores, how do we know 'cap-shape' is better than like another feature with 8 gini scores?\n",
    "    # best way is take a 'weighted-average' of values.\n",
    "    print(k, gini(subf))\n",
    "\n",
    "# weighted average for a WHOLE column.\n",
    "# returns the information gain to answering question of split in gini fxn. how can we best split to better gain info.\n",
    "s = 0\n",
    "for k, subf in df.groupby('cap-shape'):\n",
    "\n",
    "    # score of this one feature split, trying to predicy 'y' class val.\n",
    "    g = gini(subf)\n",
    "    # ^ this is score for exactly one variable\n",
    "    s += g * len(subf) / len(df)\n",
    "    # this is the 'weighted average' of the gini score, takes into effect how large that split is. how many elements\n",
    "    #\n",
    "display(s)\n",
    "\n",
    "# ^if this is smaller than the original parent nodes gini score,\n",
    "# (the default score of just the gini function) then that means we are making progress\n",
    "# no, just do pandas for rn. do numpy later once working.\n",
    "\n",
    "\n",
    "def gini_numpy(self, X, y, y_labels):\n",
    "    count = 0\n",
    "    for i in range(len(y)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refined code of above. maybe have a parameter for which scoring fxn to use?\n",
    "# this is biased towards higher cardinality sets. just by definition of gini and dividing\n",
    "# algorithm \"c.45\", sea.45?: ratio of information gain/ entropy of entire dataset. helps bias back towards lower cardinality sets.\n",
    "# this is an example of a greedy algorithm. Finding best option at this current point.\n",
    "# At each node, its a local optimization. might not be the best. Greedy.\n",
    "\n",
    "#this only does the impurity for ONE FEATURE(column).\n",
    "def impurity(df, prop):\n",
    "    s = 0\n",
    "    for k, subf in df.groupby(prop):\n",
    "        # score of this one feature split, trying to predicy 'y' class val.\n",
    "        g = gini(subf)\n",
    "        # ^ this is score for exactly one variable\n",
    "        s += g * len(subf) / len(df) # Get weighted average of gini values for a whole columns different values.\n",
    "    return s\n",
    "\n",
    "impurity(df, 'odor')\n",
    "\n",
    "# skip first one, thats our target class.\n",
    "# make this a recursive process( or stack somehow? maybe two stacks, one for node and one for list indices)\n",
    "# if continuous data, split into buckets.\n",
    "\n",
    "# Given a df/sub-df, this will find the impurities for EACH column, if we were to split on that column somehow.\n",
    "# Then we can find the one with the lowest, and split on that. \n",
    "# ?? if that column has many categories though, we should go to the lowest gini score of that selected column, right?\n",
    "# maybe if two classes are the same gini score, go with the one that is bigger? idk. or just choose first.\n",
    "s = []\n",
    "for f in fields[1:]: #MAKE sure to not grab label-column\n",
    "    # sort these values to always choose LOWEST value. if it is zero that is perfect.\n",
    "    s.append((f, impurity(df, f)))\n",
    "    print(f, impurity(df, f))\n",
    "    \n",
    "# ORIGINAL WEIGHTED GINI IMPURITY - NEW SPLIT WEIGHTED AVERAGE GINI = GINI-GAIN( how much impurity we have removed)\n",
    "# we want to maximize the gini-gain, how much that split better gives info. the best new split will always be the \n",
    "# lowest value possible( if zero, it means we have removed ALL the impurity by from dataset by splitting this way.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "397px",
    "left": "1098px",
    "right": "20px",
    "top": "119px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
